{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "\n",
    "from graph import get_ba\n",
    "from graph.utils import get_edge_list, directed2undirected\n",
    "from solver.default import rk4\n",
    "\n",
    "from agent import GraphExtractor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.ppo.ppo import PPO\n",
    "from swing_data import SwingData\n",
    "from swing_env import SwingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 20\n",
    "mean_degree = 4.0\n",
    "random_engine = np.random.default_rng(42)\n",
    "precision = np.float32\n",
    "\n",
    "# Network\n",
    "g = get_ba(num_nodes, mean_degree)\n",
    "num_edges = g.number_of_edges()\n",
    "edge_list = get_edge_list(g)\n",
    "weights = np.ones(num_edges, dtype=precision)\n",
    "\n",
    "\n",
    "# Swing parameters\n",
    "phase = random_engine.uniform(0, 2 * np.pi, num_nodes).astype(precision, copy=False)\n",
    "power = np.array([1] * int(num_nodes / 2) + [-1] * int(num_nodes / 2), dtype=precision)\n",
    "random_engine.shuffle(power)\n",
    "gamma = np.ones(num_nodes, dtype=precision)\n",
    "mass = np.ones(num_nodes, dtype=precision)\n",
    "dt = 0.001\n",
    "\n",
    "swing_data = SwingData(\n",
    "    edge_list=edge_list,\n",
    "    phase=phase,\n",
    "    dphase=np.zeros_like(phase),\n",
    "    coupling=weights,\n",
    "    power=power,\n",
    "    gamma=gamma,\n",
    "    mass=mass,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL environment\n",
    "swing_env = SwingEnv(swing_data, dt, equilibrium_step = 123)\n",
    "check_env(swing_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.4     |\n",
      "|    ep_rew_mean     | 11.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 20, 2]' is invalid for input of size 2560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m policy_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeatures_extractor_class\u001b[39m\u001b[39m\"\u001b[39m: GraphExtractor,\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeatures_extractor_kwargs\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     },\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m model \u001b[39m=\u001b[39m PPO(\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMultiInputPolicy\u001b[39m\u001b[39m\"\u001b[39m, swing_env, policy_kwargs\u001b[39m=\u001b[39mpolicy_kwargs, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:299\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    287\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    288\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    300\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    301\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    302\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    303\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    304\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    305\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    306\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    307\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    308\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:254\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    256\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    258\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:199\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_sde:\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mreset_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m--> 199\u001b[0m values, log_prob, entropy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mevaluate_actions(rollout_data\u001b[39m.\u001b[39;49mobservations, actions)\n\u001b[1;32m    200\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    201\u001b[0m \u001b[39m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/policies.py:638\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_actions\u001b[39m(\u001b[39mself\u001b[39m, obs: th\u001b[39m.\u001b[39mTensor, actions: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[th\u001b[39m.\u001b[39mTensor, th\u001b[39m.\u001b[39mTensor, th\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    629\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[39m    Evaluate actions according to the current policy,\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[39m    given the observations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m        and entropy of the action distribution.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     latent_pi, latent_vf, latent_sde \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_latent(obs)\n\u001b[1;32m    639\u001b[0m     distribution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_dist_from_latent(latent_pi, latent_sde)\n\u001b[1;32m    640\u001b[0m     log_prob \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/policies.py:581\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_latent\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39mGet the latent code (i.e., activations of the last layer of each network)\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39mfor the different networks.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m    for the actor, the value function and for gSDE function\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs)\n\u001b[1;32m    582\u001b[0m latent_pi, latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor(features)\n\u001b[1;32m    584\u001b[0m \u001b[39m# Features for sde\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/policies.py:128\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mNo features extractor was set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m preprocessed_obs \u001b[39m=\u001b[39m preprocess_obs(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, normalize_images\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_images)\n\u001b[0;32m--> 128\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor(preprocessed_obs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/swing_rl/hoyun/agent.py:35\u001b[0m, in \u001b[0;36mGraphExtractor.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observations: OrderedDict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     32\u001b[0m     num_nodes \u001b[39m=\u001b[39m observations[\u001b[39m\"\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m     failed_at_this_step \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 35\u001b[0m         observations[\u001b[39m\"\u001b[39;49m\u001b[39mfailed_at_this_step\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, num_nodes, \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     node_feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\n\u001b[1;32m     38\u001b[0m         [\n\u001b[1;32m     39\u001b[0m             observations[\u001b[39m\"\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m\"\u001b[39m],  \u001b[39m# (0, 2pi)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     47\u001b[0m     )\u001b[39m.\u001b[39msqueeze()  \u001b[39m# (N, 6)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     step \u001b[39m=\u001b[39m observations[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(num_nodes, \u001b[39m1\u001b[39m)  \u001b[39m# (N, 1)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 20, 2]' is invalid for input of size 2560"
     ]
    }
   ],
   "source": [
    "# torch_geometric\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "edge_index = directed2undirected(edge_list, device)  # (2, E)\n",
    "edge_attr = torch.unsqueeze(\n",
    "    torch.tensor(np.concatenate([weights, weights]), device=device), -1\n",
    ")  # (E, 1)\n",
    "\n",
    "policy_kwargs = {\n",
    "    \"features_extractor_class\": GraphExtractor,\n",
    "    \"features_extractor_kwargs\": {\n",
    "        \"features_dim\": num_nodes,\n",
    "        \"edge_index\": edge_index,\n",
    "        \"edge_attr\": edge_attr,\n",
    "    },\n",
    "}\n",
    "model = PPO(\"MultiInputPolicy\", swing_env, policy_kwargs=policy_kwargs, verbose=2)\n",
    "model.learn(total_timesteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4f26a53cbf74815b49292b87a256fbb46bae8ae5e1a00cdbd9c6a6050cd2ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
